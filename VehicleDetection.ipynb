{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile scene.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "class Scene:\n",
    "    \"\"\"The scene class is used to provide feature subsampling on an image. \n",
    "    \n",
    "    Rarams:\n",
    "        image: The image that features will be extracted from\n",
    "        window_size: The size of the window in units of 8 pixel by 8 pixel cells.\n",
    "        color_space: An optional color space to transform the image to before extracting features.\n",
    "        hog_color_channel: An array of channel indices that should be used when extracting hog features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image, window_size=[8,8], color_space='RGB', hog_color_channel=[0,1,2]):\n",
    "        self.gridSize = 8\n",
    "        self.feature_size = [8,8]\n",
    "        self.windowSize = np.int32(window_size)\n",
    "        self.colorSpace = color_space\n",
    "        self.originalImage = image\n",
    "        self._scale_factor = np.divide(self.windowSize, self.feature_size)\n",
    "        self.image = self.__processImage(image, color_space)\n",
    "        self.gridShape = (int(self.image.shape[0]/self.gridSize), int(self.image.shape[1]/self.gridSize))\n",
    "        self.viewPort = tuple(slice(0, int(dim/self.gridSize)+1, None) for dim in self.image.shape[0:2])\n",
    "\n",
    "        # hog setup\n",
    "        self._hog = None\n",
    "        self._orientation_bins = 9\n",
    "        self._cells_per_block=2\n",
    "        self._channel=hog_color_channel\n",
    "        \n",
    "        # color hist setup\n",
    "        self._colorHist = None\n",
    "        self._color_bins = 32\n",
    "        self.ss0 = None\n",
    "        self.ss1 = None\n",
    "        \n",
    "    def __processImage(self, image, color_space):\n",
    "        \"\"\"Scales the input image to maintain 8x8 cell feature boxes and tranforms the color space if needed. \n",
    "        The scaling is required as the size of the feature vector must remain constant between different \n",
    "        window sizes.\n",
    "        \n",
    "        Params:\n",
    "            image: The image to transform.\n",
    "            color_space: The color space to use.\n",
    "        Returns:\n",
    "            The modified image.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_size = tuple(np.int32(np.divide(image.shape[0:2], self._scale_factor)))\n",
    "        feature_image = cv2.resize(image, new_size[::-1])\n",
    "        \n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(feature_image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(feature_image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(feature_image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(feature_image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(feature_image, cv2.COLOR_RGB2YCrCb)\n",
    "        \n",
    "        return feature_image\n",
    "        \n",
    "    def __getitem__(self, arg):\n",
    "        \"\"\"Overloaded subscripting operator used to return a tuple of slice objects. This is\n",
    "        usefull when used with the viewPort property 'scene.viewPort = scene[1:2,1:2,1]'\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return arg\n",
    "        \n",
    "    def view(self):\n",
    "        \"\"\"The current view of the image.\n",
    "        \n",
    "        returns: An image that represents the current view of the overall image.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ss0 = slice(self.viewPort[0].start*self.gridSize, self.viewPort[0].stop*self.gridSize, None)\n",
    "        self.ss1 = slice(self.viewPort[1].start*self.gridSize, self.viewPort[1].stop*self.gridSize, None)\n",
    "        return self.image[self.ss0, self.ss1]\n",
    "    \n",
    "    @property\n",
    "    def viewPort(self):\n",
    "        \"\"\"Setter for viewPort property.\n",
    "        \"\"\"\n",
    "        return self._viewPort\n",
    "    \n",
    "    @viewPort.setter\n",
    "    def viewPort(self, arg):\n",
    "        \"\"\"Getter for viewPort property. Raises an exception if the argument is not a 2-tuple of slice\n",
    "        objects.\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(arg) is not tuple or len(arg) != 2:\n",
    "            raise ValueError(\"Argument must be a 2-tuple of slice objects.\")\n",
    "        self._viewPort = arg\n",
    "    \n",
    "    @property\n",
    "    def HogFeatures(self):\n",
    "        \"\"\"The hog features are created lazily for the entire input image. Once the Hog features\n",
    "        are created, the features for the current viewPort will be returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        # create the hog features only if they have not yet been created\n",
    "        if self._hog is None:\n",
    "            self._hog = []\n",
    "            \n",
    "            # create hog features for all of the requested hog channels\n",
    "            for channel in self._channel:\n",
    "                hog_channel = self.image[:,:,channel]\n",
    "                self._hog.append(hog(hog_channel, orientations=self._orientation_bins, pixels_per_cell=(self.gridSize, self.gridSize),\n",
    "                           cells_per_block=(self._cells_per_block, self._cells_per_block), transform_sqrt=True, \n",
    "                           visualise=False, feature_vector=False))\n",
    "        \n",
    "        # the Hog features have a different shape than the grid so we need to fix them up here.\n",
    "        maxHogDim0 = int(self.feature_size[0] - self._cells_per_block + 1)\n",
    "        maxHogDim1 = int(self.feature_size[1] - self._cells_per_block + 1)\n",
    "        maxStop0 = np.minimum(self.viewPort[0].stop, self.viewPort[0].start + maxHogDim0)\n",
    "        maxStop1 = np.minimum(self.viewPort[1].stop, self.viewPort[1].start + maxHogDim1)\n",
    "        hs0 = slice(self.viewPort[0].start, maxStop0 , self.viewPort[0].step)\n",
    "        hs1 = slice(self.viewPort[1].start, maxStop1 , self.viewPort[1].step)\n",
    "        \n",
    "        # ravel the current view of the hog features in a 1d vector\n",
    "        return np.hstack([hog[hs0, hs1,...] for hog in self._hog]).ravel()\n",
    "    \n",
    "    @property\n",
    "    def ColorHistFeatures(self):\n",
    "        \"\"\"The color histogram features are created lazily. The first time they are requested, a map of\n",
    "        the histogram in each grid cell is calculated and saved. After this point, calculating the histograms\n",
    "        for a given viewPort is simply a matter of suming up the histograms from the contibuting cells.\n",
    "        \"\"\"\n",
    "        \n",
    "        view = self.view()\n",
    "        hist0 = np.histogram(view[:,:,0], bins=self._color_bins, range=(0,256))\n",
    "        hist1 = np.histogram(view[:,:,1], bins=self._color_bins, range=(0,256))\n",
    "        hist2 = np.histogram(view[:,:,2], bins=self._color_bins, range=(0,256))\n",
    "        return np.vstack((hist0[0], hist1[0], hist2[0])).ravel()\n",
    "    \n",
    "    @property\n",
    "    def SpatialFeatures(self):\n",
    "        \"\"\" Creates spatial features by downsampling the current view of the image to a 32x32 image and then\n",
    "        raveling this to a 1d vector.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ss0 = slice(self.viewPort[0].start*self.gridSize, self.viewPort[0].stop*self.gridSize, None)\n",
    "        self.ss1 = slice(self.viewPort[1].start*self.gridSize, self.viewPort[1].stop*self.gridSize, None)\n",
    "        return cv2.resize(self.image[self.ss0, self.ss1], (32, 32)).ravel()\n",
    "    \n",
    "    def SearchWindows(self, i_start_stop, j_start_stop, ij_step_size, classifier):\n",
    "        \"\"\"Creates a sliding window between the provided i/j start/stop with the provided overlap\n",
    "        and searches the image for vehicles using the provided classifer. All distances/lengths should be\n",
    "        provided in units of gridSize and all coordinates are in ij order.\n",
    "        \n",
    "        Args:\n",
    "            i_start_stop: A 2-tuple that specifies the i search range as a fraction of the image height.\n",
    "            j_start_stop: A 2-tuple that specifies the j search range as a fraction of the image width.\n",
    "            ij_step_size: A 2-tuple that specifies the step sizes as fractions of the windows size.\n",
    "            classifier: The classifier to use when testing if a given cell for is a vehicle or not.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the span of the region to be searched in i/j\n",
    "        h,w = self.image.shape[0:2]\n",
    "        i_start_stop = (np.float32(i_start_stop)*h/self.gridSize).astype(np.int32)\n",
    "        j_start_stop = (np.float32(j_start_stop)*w/self.gridSize).astype(np.int32)\n",
    "        ij_span = np.int32([i_start_stop[1] - i_start_stop[0], j_start_stop[1] - j_start_stop[0]])\n",
    "\n",
    "        # Compute the number of pixels per step in x/y\n",
    "        ij_step = np.multiply(self.feature_size, ij_step_size)\n",
    "        \n",
    "        # compute the number of windows in x/y\n",
    "        ij_num_windows = np.int32((ij_span - self.feature_size) / ij_step + 1)\n",
    "        \n",
    "        # build the windows\n",
    "        all_windows = []\n",
    "        hot_windows = []\n",
    "        for i in range(ij_num_windows[0]):\n",
    "            for j in range(ij_num_windows[1]):\n",
    "                \n",
    "                # the top-left and bottom-right points are constructed in ij order and added to the list\n",
    "                tl = np.int32([i*ij_step[0] + i_start_stop[0], j*ij_step[1] + j_start_stop[0]])\n",
    "                br = np.int32(tl + self.feature_size)\n",
    "                window = (tuple(tl), tuple(br))\n",
    "                all_windows.append(window)\n",
    "                \n",
    "                # set the viewport on the scene (self) and then calculate the features\n",
    "                self.viewPort = self[tl[0]:br[0], tl[1]:br[1]]        \n",
    "                spatial = self.SpatialFeatures\n",
    "                colorHist = self.ColorHistFeatures\n",
    "                hogFeatures = self.HogFeatures\n",
    "                features = np.concatenate((spatial, colorHist, hogFeatures))  \n",
    "                test_features = classifier.X_scaler.transform(features.reshape(1, -1))\n",
    "                prediction = classifier.Predict(test_features)\n",
    "        \n",
    "                # if a car is predicted, the window goes in the hot_windows list\n",
    "                if prediction == 1:\n",
    "                    hot_windows.append(window)\n",
    "         \n",
    "        # return hot windows and all windows with associated scale factors\n",
    "        return (self._scale_factor, hot_windows), (self._scale_factor, all_windows)\n",
    "    \n",
    "    def DrawWindows(self, window_sets):\n",
    "        \"\"\"Draws the provided window sets on the image.\n",
    "        \n",
    "        Params:\n",
    "            window_sets: The windows sets to draw. Each set is a 2-tuple composed of a scale and a list of windows\n",
    "            to draw at that scale. (scale, [windows])\n",
    "        Returns:\n",
    "            A copy of the original image with the windows sets drawn on it.\n",
    "        \"\"\"\n",
    "        \n",
    "        draw_img = np.copy(self.originalImage)\n",
    "        for scale, windows in window_sets:\n",
    "            scalei, scalej = scale[0]*self.gridSize, scale[1]*self.gridSize\n",
    "            for window in windows:\n",
    "                points = ((int(window[0][1]*scalej), int(window[0][0]*scalej)),(int(window[1][1]*scalei), int(window[1][0]*scalei)))\n",
    "                cv2.rectangle(draw_img, points[0], points[1], (0,0,1.0), 6)\n",
    "        return draw_img\n",
    "    \n",
    "    def ContributeHeat(self, heatmap, window_sets, threshold=0):\n",
    "        \"\"\"Uses the provided window sets to add heat to the provided heat map.\n",
    "        \n",
    "        Params:\n",
    "            heatmap: The heatmap to add to.\n",
    "            window_sets: The windows sets to add to. Each set is a 2-tuple composed of a scale and a list of windows\n",
    "            to draw at that scale. (scale, [windows])\n",
    "            threshold: An optional threshold to apply to the heatmap after adding contributions. \n",
    "        Returns:\n",
    "            The heatmap with contributions from the provided window sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        for scale, windows in window_sets:\n",
    "            scalei, scalej = scale[0]*self.gridSize, scale[1]*self.gridSize\n",
    "            for window in windows:\n",
    "                points = ((int(window[0][0]*scalej), int(window[0][1]*scalej)),(int(window[1][0]*scalei), int(window[1][1]*scalei)))\n",
    "                heatmap[points[0][0]:points[1][0], points[0][1]:points[1][1]] += 1\n",
    "                \n",
    "        np.clip(heatmap, 0, 255)\n",
    "        heatmap[heatmap < threshold] = 0\n",
    "        return heatmap\n",
    "    \n",
    "    def DrawHeatMap(self, window_sets, threshold=0):\n",
    "        \"\"\"Creates a heatmap with the provided window sets.\n",
    "        \n",
    "        Params:\n",
    "            window_sets: The windows sets to add to. Each set is a 2-tuple composed of a scale and a list of windows\n",
    "            to draw at that scale. (scale, [windows])\n",
    "            threshold: An optional threshold to apply to the heatmap. \n",
    "        Returns:\n",
    "            The heatmap with contributions from the provided window sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        heatmap = np.zeros_like(self.originalImage[:,:,0]).astype(np.float32)\n",
    "        return self.ContributeHeat(heatmap, window_sets, threshold)\n",
    "    \n",
    "    def GetLabeledRegions(self, window_sets=None, heatmap=None, threshold=0, visualize=False):\n",
    "        \"\"\"Gets labeled regions from the provided window sets or heatmap.\n",
    "        \n",
    "        Params:\n",
    "            window_sets: The windows sets used to create a heatmap to convert to labeled regions. Each set is \n",
    "            a 2-tuple composed of a scale and a list of windows to draw at that scale. (scale, [windows]). This \n",
    "            parameter may be None if a heatmap is provided.\n",
    "            heatmap: A heatmap to create the labeled regions from. This parameter may be None if windows_sets are\n",
    "            provided. If both are set, this argument takes precedence.\n",
    "            threshold: An optional threshold to apply to the heatmap before the labeled regions are calculated.\n",
    "        Returns:\n",
    "            A 2-tuple with the scale and detected regions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # if the caller did not provide a heatmap then we create one from the window_sets \n",
    "        if heatmap is None:\n",
    "            if window_sets is None:\n",
    "                raise ValueError('A heatmap or window_sets must be provided.')\n",
    "            heatmap = self.DrawHeatMap(window_sets, threshold)\n",
    "            \n",
    "        heatmap[heatmap < threshold] = 0\n",
    "            \n",
    "        # get the labeled image\n",
    "        label_img, num_labels = label(heatmap)\n",
    "        \n",
    "        # find the top left (tl) and bottom right (br) points that contain the labeled regions\n",
    "        detected_regions = []\n",
    "        for region_label in range(1, num_labels+1):\n",
    "            region_i, region_j = (label_img == region_label).nonzero()\n",
    "            tl = (np.amin(region_i), np.amin(region_j))\n",
    "            br = (np.amax(region_i), np.amax(region_j))\n",
    "            detected_regions.append((tl, br))\n",
    "        \n",
    "        # the heat map is already at a scale that matches the original image\n",
    "        retVal = ([1.0/self.gridSize, 1.0/self.gridSize], detected_regions)\n",
    "        if visualize is True:\n",
    "            retVal = (retVal, label_img)\n",
    "            \n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile classifier.py\n",
    "\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"A class to help train and persist a classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.accuracy = None\n",
    "        self.classifier = None\n",
    "        self.X_scaler = None\n",
    "    \n",
    "    def Train(self, X, y, test_split=0.2, save_to=None):\n",
    "        \"\"\"Trains the labeled data provided and checks accuracy against the test data.\n",
    "        \n",
    "        Args:\n",
    "            X: The training data.\n",
    "            y: The training labels.\n",
    "            test_split: The fraction of test data to split off from the training data.\n",
    "            save_to: If provides, the classifier will be saved to the provided location.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fit a per-column scaler\n",
    "        self.X_scaler = StandardScaler().fit(X)\n",
    "        \n",
    "        # Apply the scaler to X\n",
    "        scaled_X = self.X_scaler.transform(X)\n",
    "\n",
    "        # Split up data into randomized training and test sets\n",
    "        rand_state = np.random.randint(0, 100)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=test_split, random_state=rand_state)\n",
    "        \n",
    "        # use a linear SVC classifier\n",
    "        self.classifier = LinearSVC()\n",
    "        \n",
    "        # measure how long it takes to train the classifier\n",
    "        t=time.time()\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        t2 = time.time()\n",
    "        print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "            \n",
    "        self.accuracy = round(self.classifier.score(X_test, y_test), 4)\n",
    "        print('Test accuracy = ', self.accuracy)\n",
    "        \n",
    "        if save_to is not None:\n",
    "            joblib.dump((self.classifier, self.X_scaler, self.accuracy), save_to, compress=9)\n",
    "            print('Saved trianed classifier to ', save_to)\n",
    "        \n",
    "    def Predict(self, X):\n",
    "        \"\"\"Predicts if the given samples are vehicles or not.\n",
    "        \n",
    "        Args:\n",
    "            X: The samples.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.classifier.predict(X)\n",
    "    \n",
    "    def Load(self, load_from):\n",
    "        \"\"\"Load the classifier from the provided file.\n",
    "        \n",
    "        Args:\n",
    "            load_from: The file to load the classifier from.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classifier, self.X_scaler, self.accuracy = joblib.load(load_from)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile detector.py\n",
    "\n",
    "from scene import Scene\n",
    "from classifier import Classifier\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class VehicleDetector:\n",
    "    \"\"\"A class to help maintain state between video frames.\n",
    "    \n",
    "    Args:\n",
    "        classifier: The classifier to use to process video frames.\n",
    "        heatmap_size: The size of the heatmap to be used.\n",
    "        memory: The number of previous frames to take into account for thresholding.\n",
    "        color_space: The color_space to use when classifying frames.\n",
    "        hog_color_channel: The color channels to use for hog features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifier, heatmap_size=(720, 1280), memory=3, threshold=2, color_space='YCrCb', hog_color_channel=[0,1,2]):\n",
    "        self.classifier = classifier\n",
    "        self.heatmapSize = heatmap_size\n",
    "        self.memory = memory\n",
    "        self.threshold = threshold\n",
    "        self.colorSpace = color_space\n",
    "        self.hogColorChannel = hog_color_channel\n",
    "        self.heatmapQueue = self._initializeQueue(self.memory)\n",
    "        self.previousRegions = None\n",
    "        self.hotWindows = None\n",
    "        \n",
    "    def _initializeQueue(self, memory):\n",
    "        \"\"\"Initializes the deque used to hold previous heatmaps.\n",
    "        \n",
    "        Args:\n",
    "            memory: The number of previous frames to hold in the queue.\n",
    "        \"\"\"\n",
    "        \n",
    "        queue = deque([])\n",
    "        for _ in range(memory):\n",
    "            queue.appendleft(np.zeros(shape=self.heatmapSize, dtype=np.float32))\n",
    "        return queue\n",
    "    \n",
    "    def ProcessFrame(self, frame):\n",
    "        \"\"\"Searches for vehicles in the provided video frame and draws their bounding boxes.\n",
    "        \n",
    "        Args:\n",
    "            frame: The video frame to process.\n",
    "        \"\"\"\n",
    "        \n",
    "        # the frame needs to be converted to the format that was used during training: float [0.0, 1.0]\n",
    "        frame = frame.astype(np.float32) / 255.0\n",
    "        self.hotWindows = []\n",
    "        \n",
    "        window_sizes = ((6,6), (10,10), (12,12))\n",
    "        overlaps = ((0.25,0.25),(0.125,0.125), (0.125,0.125))\n",
    "        y_ranges = ((0.55, 0.7), (0.55, 0.8), (0.5, 0.95)) \n",
    "        x_ranges = ((0.5, 1.0), (0.5, 1.0), (0.4, 1.0))\n",
    "\n",
    "        # loop through all window sizes and search the image\n",
    "        for window, overlap, y_range, x_range in zip(window_sizes, overlaps, y_ranges, x_ranges):\n",
    "            scene = Scene(frame, window_size=window, color_space=self.colorSpace, hog_color_channel=self.hogColorChannel)\n",
    "            h, a = scene.SearchWindows(y_range, x_range, overlap, self.classifier)\n",
    "            self.hotWindows.append(h)\n",
    "        \n",
    "        # get the heatmap for the current frame and add it to the historical frames\n",
    "        heatmap = scene.DrawHeatMap(self.hotWindows)\n",
    "        self.heatmapQueue.appendleft(np.zeros(shape=self.heatmapSize, dtype=np.float32))\n",
    "        for hmap in self.heatmapQueue:\n",
    "            hmap += heatmap\n",
    "        \n",
    "        # finally, get the detected regions from the multi-frame heatmap\n",
    "        detected_regions = scene.GetLabeledRegions(heatmap=self.heatmapQueue.pop(), threshold=self.threshold)\n",
    "        detected_regions = self._sanitizeRegions(detected_regions)\n",
    "        \n",
    "        # extract the windows from the detected_regions and low-pass filter them WRT time\n",
    "        if self.previousRegions is not None and len(self.previousRegions) == len(detected_regions[1]):\n",
    "            try:\n",
    "                filtered_regions = self._filterRegions(detected_regions)\n",
    "                out_frame = scene.DrawWindows([filtered_regions])\n",
    "                self.previousRegions = filtered_regions[1]\n",
    "            except ValueError:\n",
    "                out_frame = scene.DrawWindows([detected_regions])\n",
    "                self.previousRegions = detected_regions[1]\n",
    "        else:\n",
    "            # there is a mis-match in number of detected regions so just ignore the previous regions\n",
    "            out_frame = scene.DrawWindows([detected_regions])\n",
    "            self.previousRegions = detected_regions[1]\n",
    "          \n",
    "        # format the return image for video stream\n",
    "        return np.uint8(out_frame * 255)\n",
    "    \n",
    "    def GetHeatMap(self):\n",
    "        return self.heatmapQueue[-1]\n",
    "    \n",
    "    def DrawPositiveDetections(self, frame):\n",
    "        scene = Scene(frame)\n",
    "        return scene.DrawWindows(self.hotWindows)\n",
    "    \n",
    "    def GetLabelsMap(self):\n",
    "        scene = Scene(frame)\n",
    "    \n",
    "    def _sanitizeRegions(self, regions):\n",
    "        sanitary = []\n",
    "        scale, windows = regions\n",
    "        for window in windows:\n",
    "            w,h = window[1][1] - window[0][1], window[1][0] - window[0][0] \n",
    "            if w > 64 and h > 64:\n",
    "                sanitary.append(window)\n",
    "        return (scale, sanitary)\n",
    "    \n",
    "    def _filterRegions(self, detected_regions):\n",
    "        filtered_regions = []\n",
    "        scale, regions = detected_regions\n",
    "\n",
    "        # iterate through the detected regions and find the matching region from the previous frame.\n",
    "        for i, region in enumerate(regions):\n",
    "            previous_region = self._matchRegion(region)\n",
    "            filtered_regions.append(self._filterRegion(region, previous_region, 0.3))\n",
    "\n",
    "        return (scale, filtered_regions)\n",
    "                   \n",
    "    def _filterRegion(self, new, old, new_factor):\n",
    "        \n",
    "        out = []\n",
    "        new_factor, old_factor = new_factor, 1 - new_factor\n",
    "        for n, o in zip(new, old):\n",
    "            ret = (new_factor*n[0] + old_factor*o[0], new_factor*n[1] + old_factor*o[1])\n",
    "            out.append(ret)\n",
    "        return tuple(out)\n",
    "    \n",
    "    def _matchRegion(self, region):\n",
    "        \n",
    "        # find the center point of the new region\n",
    "        cpi, cpj = (region[1][0] - region[0][0]) / 2, (region[1][1] - region[0][1]) / 2 \n",
    "        \n",
    "        # iterate through the previous regions and find the first one that contains the new cp\n",
    "        for pr in self.previousRegions:\n",
    "            tl, br = pr\n",
    "            if tl[0] < cpi and tl[1] < cpj and br[0] > cpi and br[j] > cpj:\n",
    "                return pr\n",
    "        \n",
    "        raise ValueError('Could not find a matching region is the previous regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import time\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# A helper function to extract the training features from an image\n",
    "def extractFeatures(images):\n",
    "    \n",
    "    features = []\n",
    "    oneP = len(images)/100\n",
    "    for i, image in enumerate(images):\n",
    "        \n",
    "        # report progress every 1000 images processed\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        # create the scene object and then extract the spatial, colorHist and hog features\n",
    "        scene = Scene(mpimg.imread(image), color_space='YCrCb', hog_color_channel=[0,1,2])\n",
    "        spatial = scene.SpatialFeatures\n",
    "        colorHist = scene.ColorHistFeatures\n",
    "        hogFeatures = scene.HogFeatures\n",
    "        feature = np.concatenate((spatial, colorHist, hogFeatures))\n",
    "        features.append(feature)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def trainClassifier():\n",
    "    \n",
    "    # Read in car and non-car images\n",
    "    cars = glob.glob('./data/vehicles/*/*.png')\n",
    "    notcars = glob.glob('./data/non-vehicles/*/*.png')\n",
    "\n",
    "    # extract all of the features from the images\n",
    "    print('extracting features...')\n",
    "    car_features = extractFeatures(cars)\n",
    "    notcar_features = extractFeatures(notcars)\n",
    "\n",
    "    # Create feature vectors array stacks for X and y\n",
    "    print('building X and y...')\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)\n",
    "    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "    # train a classifier and save it\n",
    "    classifier = Classifier()\n",
    "    classifier.Train(X, y, save_to='./YCrCb_AllHog_Classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile pipeline.py\n",
    "\n",
    "from classifier import Classifier\n",
    "from detector import VehicleDetector\n",
    "from scene import Scene\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.Load('./basic_classifier.pkl')\n",
    "detector = VehicleDetector(classifier, memory=20, threshold=60, color_space='YCrCb', hog_color_channel=[0,1,2])\n",
    "\n",
    "projectVideo = './udacity/project_video.mp4'\n",
    "projectVideoOut = './udacity/project_video_out9.mp4'\n",
    "clip1 = VideoFileClip(projectVideo)\n",
    "out_clip = clip1.fl_image(detector.ProcessFrame)\n",
    "%time out_clip.write_videofile(projectVideoOut, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
